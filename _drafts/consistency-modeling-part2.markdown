---
layout: post
title: "Minimum Consistency Models pt. 2"
tags: [ml, math]
excerpt: >
  A minimum implementation of consistency models.
---

This is an elaboration upon a talk I gave at [K-Scale Labs](https://www.ycombinator.com/companies/k-scale-labs) with these [slides](https://docs.google.com/presentation/d/18dlGr7veUm9JJdOz9r6l0rRYYDlzWDRs6uF6lraZs3w/edit?usp=sharing) and this [GitHub repo](https://github.com/kscalelabs/min-consistency-models).

If you haven't already, look at [Minimum Consistency Models pt. 1](/consistency-modeling-part1) first to get an overview of how diffusion modeling and flow matching work. Consistency models have been especially important in machine learning applications where quick inference is necessary (e.g. robotics!), and here we will go over a minimum "hackable" implementation of thse models

## Papers + Resources
Some sources to look for more insight + math:
- [Flow Matching for Generative Modeling](https://arxiv.org/pdf/2210.02747)
- [Consistency Models](https://arxiv.org/pdf/2303.01469)
- [Diffusion Models from Scratch](https://www.tonyduan.com/diffusion/index.html)
- [What are Diffusion Models?](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

## The Idea

As stated previously, given the optimal transport path between some point generated by noise, we want to be able to step towards a goal image in as minimal steps as possible. In order to do this, we have to have an interesting approach: we want to make it so that all points along our defined flow path result in the same image, regardless of whether it's early along the path or late.

![consistency_models](/images/consistency-modeling/consistency_models.webp)

This seems very interesting, reminiscent of consistency losses in StyleGAN or CycleGAN. Of course it seems like it would make our generated images more consistent, but how does it make the inference *quicker*?