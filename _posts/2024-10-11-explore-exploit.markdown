---
layout: post
title: "The Explore vs. Exploit Dilemma"
tags: [rl, startup, musings]
excerpt: > 
  Multi-armed bandits and a prolonged analogy
---

{% katexmm %}

I often analogize some real-world problems according to their ML-related counterparts. One of them is the exporation-exploitation problem, but it's often been met with minimal recognition. I wrote this blog as something I can refer to the next time I get a "what do you mean?"

## Introduction: The Multi-Armed Bandit Problem

Suppose we have a series of decisions to make, each with the potential to yield a reward. In our multi-armed bandit problem, we aim to develop a strategy to maximize this reward over time. We envision each "arm" as a slot machine, each one hiding a different reward distribution. Our task is to identify which arm to pull at each step in time to accumulate the most reward.

If we consider $t=0$ as our starting state—knowing nothing about the reward distributions—and $t=1$ as the ideal state—where we have complete knowledge of the best arm—then we can define a function between ignorance and an optimal selection. In this framework, we can imagine a vector field guiding us from exploring new arms to exploiting the most rewarding ones.

Our state of knowledge at time $t$ can be denoted $\phi_t(x)$, representing our expected reward for each arm, updated after each trial. We can define the expected reward flow $\phi_t(x)$ as:

$$\phi_t(x) = (1 - \epsilon_t) \, Q(x) + \epsilon_t \, U(x)$$

where $\epsilon_t$ is an exploration parameter that decreases over time, $Q(x)$ is the action-value function for each arm (capturing the expected reward given our current understanding), and $U(x)$ is a measure of the uncertainty or unexplored potential for each arm. 

In early steps, $t=0$, our strategy should be primarily **exploratory**: $\epsilon_0 = 1$ and thus $\phi_0(x) \approx U(x)$. On the contrary, as $t$ approaches 1, $\epsilon_t$ should approach 0, directing $\phi_t(x)$ toward the maximum expected value action, or **exploitative** policy. To represent this transition, we can set $\epsilon_t = 1 - \beta t$, where $\beta$ is a decay constant that controls how quickly we shift from exploration to exploitation.

The derivative of this function $\phi$ defines a *policy gradient*, guiding our choice of action. Instead of learning the policy directly, we train **a forward dynamics model** $f_\theta$ to predict rewards given our current knowledge, aiming to maximize:

$$\nabla_\theta J(\theta) = \mathbb{E} \Big[ \sum_{t=0}^{T} \gamma^t R_t \Big]$$

where $\gamma$ is the discount factor controlling the impact of future rewards, and $R_t$ is the observed reward at each time step $t$. This function captures our goal to maximize cumulative rewards while balancing exploration and exploitation over time.

The model’s predictions thus guide us to iteratively pull arms that will yield the highest cumulative rewards, refining our understanding of each arm’s distribution and honing in on optimal actions.

## The Forward Dynamics Model

In the multi-armed bandit problem, a forward dynamics model $f_\theta$ is an auxiliary model that predicts the expected reward for each arm based on past actions and observed rewards. By approximating the environment's response to each action, $f_\theta$ helps us make informed choices, directing our strategy toward higher rewards.

#### 1. **Defining the Forward Dynamics Model**

The forward dynamics model $f_\theta$ can be structured as a parameterized function with weights $\theta$, taking in a feature vector $x$ (representing each arm's current state) and outputting a predicted reward $\hat{R}$. The model aims to approximate the action-value function $Q(x)$, which maps each action (arm) to an expected reward. In essence, $f_\theta$ estimates:

$$
f_\theta(x) \approx Q(x) = \mathbb{E}[R | x]
$$

where $R$ is the actual reward received from pulling the arm represented by $x$. Training $f_\theta$ involves refining this approximation over multiple trials, improving its ability to generalize from limited samples.

#### 2. **Training Objective**

To train $f_\theta$, we minimize the error between predicted rewards $\hat{R}$ and observed rewards $R$. The model is typically trained by minimizing the mean squared error (MSE) over all arms and trials:

$$
L(\theta) = \frac{1}{N} \sum_{i=1}^N \left( R_i - f_\theta(x_i) \right)^2
$$

where $N$ is the number of training samples, each associated with an action $x_i$ (one of the arms) and an observed reward $R_i$. Minimizing this loss function $L(\theta)$ encourages $f_\theta$ to make accurate reward predictions, allowing the model to distinguish more promising arms from less rewarding ones.

#### 3. **Data Collection Strategy**

Training the forward dynamics model requires collecting data on observed rewards from pulling each arm. However, the exploration-exploitation dilemma influences how we gather this data:

   - **Exploration Phase**: During exploration, the model pulls different arms at random to collect a diverse dataset of reward outcomes. This phase ensures that $f_\theta$ samples from a wide range of potential rewards, even from suboptimal arms, capturing enough data to model the environment’s variability.

   - **Exploitation Phase**: As the model shifts toward exploitation, it begins to pull the arms predicted to yield higher rewards based on $f_\theta$’s predictions. During this phase, $f_\theta$ refines its understanding of the best arms and focuses on modeling the nuances in expected rewards for these higher-value actions.

<!-- #### 4. **Updating the Forward Dynamics Model with New Observations**

As new reward data becomes available after each action, we update the forward dynamics model incrementally. A simple approach is to fine-tune the model after each step using gradient descent:

$$
\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)
$$

where $\alpha$ is the learning rate. This process adjusts $\theta$ to minimize the difference between predicted and observed rewards, allowing the model to adapt quickly to new data.

In cases where the environment is highly dynamic or non-stationary, we may use *replay memory* or *online learning* strategies to ensure $f_\theta$ adapts to recent patterns without forgetting earlier observations. For instance, instead of training on the entire dataset, we can sample a batch of recent observations for each update to maintain adaptability.

#### 5. **Exploration Through Uncertainty Estimation**

One powerful approach to exploration is to incorporate uncertainty estimation into $f_\theta$. Bayesian models, or models trained with dropout or ensemble techniques, can provide a measure of uncertainty for each reward prediction. For instance, if the model is unsure about the rewards of a certain arm, we could interpret this uncertainty as a signal to explore that arm further.

For example, suppose $f_\theta$ is an ensemble of models each predicting the reward $\hat{R}$ for a given arm. The variance in these predictions across the ensemble reflects the uncertainty of the model:

$$
\text{Uncertainty}(x) = \frac{1}{K} \sum_{k=1}^K \left( \hat{R}_k(x) - \frac{1}{K} \sum_{j=1}^K \hat{R}_j(x) \right)^2
$$

where $K$ is the number of ensemble models. If this uncertainty is high, the model can be designed to prioritize exploration for that arm, gathering more data to improve its predictions. -->

#### 4. **Incorporating Reward Predictions into Policy Gradients**

Once trained, $f_\theta$’s reward predictions are used to inform the policy and guide our decision-making. The policy gradient objective, which maximizes cumulative rewards, can be modified to include predicted rewards:

$$
\nabla_\theta J(\theta) = \mathbb{E} \left[ \sum_{t=0}^{T} \gamma^t f_\theta(x_t) \right]
$$

where $\gamma$ is the discount factor for future rewards, $x_t$ is the action-state at time $t$, and $f_\theta(x_t)$ provides the reward prediction. This objective encourages the model to pull arms with higher predicted rewards, continually adjusting based on new data from $f_\theta$.

Ultimately, the forward dynamics model $f_\theta$ enables a structured approach to decision-making by predicting rewards for each arm, balancing exploration and exploitation based on estimated reward distributions. With ongoing training, $f_\theta$ adapts to new information, ensuring the model’s predictions remain accurate as we refine our understanding of each arm. This iterative learning process helps maximize cumulative rewards by aligning actions with $f_\theta$’s growing knowledge of the environment.


## The Exploration-Exploitation Dilemma
The exploration-exploitation dilemma is central to multi-armed bandit problems: we need to *explore* enough to understand each arm's potential while also *exploiting* known information to maximize immediate rewards. The parameter $\beta$, which governs the rate at which we shift from exploration to exploitation, plays a key role in this balance.

Choosing $\beta$ is a careful process. If $\beta$ is too large, $\epsilon_t$ decreases rapidly, leading the model to quickly favor exploitation. In this case, we may prematurely commit to arms that appear optimal based on limited early data, potentially missing out on higher-reward options that remain unexplored. On the other hand, if $\beta$ is too small, $\epsilon_t$ remains high, and we may spend too much time exploring, failing to capitalize on accumulated knowledge to maximize rewards.

To choose an optimal $\beta$, consider the following factors:

1. **Expected Variance Across Arms**: 
   If the rewards from each arm vary significantly, then a smaller $\beta$ is generally favorable because it allows more exploration. This ensures that our policy gathers enough information to accurately assess the best arm. On the contrary, if we expect rewards to be fairly consistent across arms, we may prefer a larger $\beta$, quickly shifting to exploitation since exploration is less likely to yield drastically different results.

2. **Risk Tolerance**:
   If our strategy prioritizes high-risk, high-reward outcomes, a smaller $\beta$ allows more exploration, potentially discovering arms with rare but substantial rewards. Conversely, if the strategy is risk-averse, a larger $\beta$ may be preferable, reducing the time spent on uncertain options and focusing on the most reliable rewards found early on.

3. **Adaptive $\beta$ Adjustments**:
   In complex environments, an adaptive approach to $\beta$ can be beneficial. Here, $\beta$ might dynamically adjust based on observed reward distributions or changes in the variance of $Q(x)$. For instance, if new arms produce high variance in $Q(x)$, $\beta$ could decrease temporarily to encourage exploration; once variance stabilizes, $\beta$ can increase to favor exploitation.

There are also added considerations, such as the total number of trials $T$and the decay function used to set $\epsilon_t$. Depending on how critical initial exploration is or how smooth one wants to transition between exploration and exploitation, we may choose a different decay function, such as exponential or reciprocal. If $T$ is large, we can afford to spend more time exploring, and thus a smaller $\beta$ is suitable. However, if $T$ is limited, we should increase $\beta$ to favor quicker convergence toward exploitation. 

Overall, the goal is to maximize cumulative reward over time by balancing exploration and exploitation. The parameter $\beta$ determines this balance by controlling the rate of decay in the exploration factor $\epsilon_t$. Experimenting with different values of $\beta$ and decay functions is often necessary to find a balance that aligns with the specific requirements of the problem.

By fine-tuning $\beta$ and $\epsilon_t$, we guide the model toward the most rewarding actions while avoiding premature convergence on suboptimal options.

## The Prolonged Analogy

I find the many people I know go through the exploration-exploitation dilemma. No one wants to be Esther Greenwood, but neither does anyone want to dive completely into some repetitive FANG lifestyle, living somebody else's dream before they've even discovered their own. How do you choose when to explore and when to exploit?

Surely, we should have an adaptive $\beta$ that depends on our surroundings. How does your $\beta$ change as all the people around you found B2B SaaS startups (no shade, simply not my kind of intellectual stimulation), go into finance, and marry college lovers? Can I bet on this arm and go all in, or should I hedge my bets and let my forward dynamics model better-model the environmental variability?

Uncertainty can be modeled as

$$
\text{U}(x) = \frac{1}{K} \sum_{k=1}^K \left( \hat{R}_k(x) - \frac{1}{K} \sum_{j=1}^K \hat{R}_j(x) \right)^2
$$

where $K$ is the number of ensemble models and $\hat{R}_k(x)$ is the reward prediction from forward dynamics model $k$. We unfortunately do not have the benefit of an ensemble, but we can utilize people who are close to us (and hopefully similar in thought process) to provide a proxy for uncertainty.

I have done a lot of exploring all throughout high school, studying many different scientific subjects for various olympiad competitions, working in various research groups, and having a general exposure to what the final $t=1$ exploitative policy would look like for many possible arms, whether SWE, quant, or academia. Additionally, I feel that exploration is a part of my reward function. An entropy parameter.

So, culminating everything, my dream would be industry research or founding a successful research-heavy startup. You don't just have a sizeable salary and prestiege to move into any other field you are interested in, but also are continually growing your personal capital through learning/exploration. I also have less uncertainty than most, given that most people I know believe I can do industry research or tackle difficult deeptech problems within a startup. Above all, I am building towards a relatively high risk tolerance --- I am okay if a startup fails (it won't). Regardless, I have a pretty long-range $T$.

I was going to make up a decay function for my $\epsilon_t$, but the truth is, it's probably something like

$$\epsilon_t = \text{Intuition}(t, \beta)$$

Thanks for reading!

<!-- # Balancing Learning and Application: A Personal Exploration

Recently, I’ve been grappling with a dilemma that feels at once personal and universally familiar: the tension between my drive to continuously learn and the need to apply that knowledge in ways that the world deems "productive." This is not solely a question of career growth; it strikes at the core of what makes a life genuinely fulfilling.

Learning, to me, is more than a stepping stone. It is essential to my sense of purpose and growth. Acquiring knowledge, wrestling with complex ideas, or deconstructing tough problems is inherently enriching. There is a purity to learning for its own sake—a pursuit where each problem solved or concept grasped represents an achievement in self-actualization that goes beyond immediate applicability.

Yet the reality is that the world often demands tangible, actionable results. The ability to turn this knowledge into something concrete, to translate learning into measurable outcomes, is what permits me to continue the pursuit of knowledge. This balance becomes almost a kind of resource-allocation problem, one that I sometimes think of as a personal variant of the "multi-armed bandit" problem: I must weigh the intrinsic value of learning against the extrinsic rewards that enable me to keep learning in the first place.

A recent experience encapsulates this tension: frequent calls and meetings that often end with, “Let’s hop on a quick call.” There’s a momentary feeling of productivity that follows. But over time, as these calls accumulate, I find myself wondering whether they’ve actually moved me forward. Have I really used my mind in any meaningful way, or have I merely ticked off another box on someone else’s checklist? After enough time, it feels as if this pattern might gradually erode my capacity for deep, complex thinking—replacing my sense of intellectual challenge with routine and surface-level engagement.

This raises a larger question about usefulness. How “useful” do I want my learning to be? It’s easy to imagine becoming deeply specialized in a niche field, pushing the limits of human understanding. There is a certain nobility in this—solving difficult, highly specialized problems is valuable. But if my expertise remains isolated from the world, am I truly realizing my potential? 

On the other hand, if I constantly chase practical applications, I risk sacrificing depth for breadth, prioritizing immediate utility over foundational understanding. Balancing these elements is challenging, and I believe that the optimal approach shifts over time and depends on the individual. 

Perhaps what I am actually striving for is to optimize a larger objective function—one that harmonizes personal growth, societal contribution, and individual fulfillment. It’s not simply a matter of choosing between pure learning and practical application but of seeking a synergy between the two. Can I channel my learning in ways that organically lead to real-world impact? And conversely, can I engage with practical work in ways that deepen my understanding?

Lately, I’ve found myself considering a concept I call “productive curiosity.” It’s an idea that sits at the intersection of my desire to learn and the need for relevance. Productive curiosity is about channeling learning into areas that offer potential for broader impact without diluting the joy of discovery. This approach is fluid, requiring ongoing adjustment and reflection.

As I continue navigating this tension, I find value in the journey itself. Every subject I dive into, every attempt to apply my knowledge, is a step in this personal resource-allocation problem. The rewards—while sometimes delayed or indirect—accumulate over time, shaping my career, my identity, and my contributions to the world.

Ultimately, perhaps the most valuable skill is meta-learning: the ability to understand how to learn, apply, and balance these aspects in a way that feels true to my path. It’s a long-term process, filled with uncertainties, yet rich with possibility. 

I don’t expect to find a final answer, but I am committed to this exploration. It’s not a glamorous journey; it’s a deliberate, practical challenge. And in many ways, the questioning, adjusting, and growing may be the most meaningful part of the experience. 

[^1]: In modeling the multi-armed bandit, each “arm” could represent a different strategy balancing learning and application. We might define the reward function as $R(a_t) = \alpha I(a_t) + (1 - \alpha) E(a_t)$ where $I(a_t)$ reflects the intrinsic reward of a learning-focused action $a_t$, $E(a_t)$ the extrinsic reward of applying knowledge, and $\alpha$ is a weight balancing the two.

[^2]: Managing this balance between exploration (learning) and exploitation (application) could involve a process like Thompson Sampling, where we continually update our beliefs about the value of different strategies based on observed outcomes $P(a_t = a) = P(\theta_a = \max_i \theta_i)$ where $\theta_a$ represents our current estimate of the value of action $a$, refined after each experience. -->


{% endkatexmm %}
